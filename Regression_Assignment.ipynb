{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment Questions-"
      ],
      "metadata": {
        "id": "xcFkKtaJLDv8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.what is simple lineaar regression ?**\n",
        "\n",
        "-->Simple linear regression aims to find a linear relationship to describe the correlation between an independent and possibly dependent variable. The regression line can be used to predict or estimate missing values, this is known as interpolation."
      ],
      "metadata": {
        "id": "i7rcie54LPmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2.What are the key assumptions of simple linear regression ?**\n",
        "\n",
        "-->Assumptions of simple linear regression are-\n",
        "1. Linearity: The relationship between\n",
        " and\n",
        " must be linear.\n",
        "Check this assumption by examining a scatterplot of x and y.\n",
        "\n",
        "\n",
        "2. Independence of errors: There is not a relationship between the residuals and the Y\n",
        " variable; in other words,\n",
        " Y is independent of errors.\n",
        "Check this assumption by examining a scatterplot of “residuals versus fits”; the correlation should be approximately 0. In other words, there should not look like there is a relationship.\n",
        "\n",
        "3. Normality of errors: The residuals must be approximately normally distributed.\n",
        "Check this assumption by examining a normal probability plot; the observations should be near the line. You can also examine a histogram of the residuals; it should be approximately normally distributed.\n",
        "\n",
        "4. Equal variances: The variance of the residuals is the same for all values of X.\n",
        "Check this assumption by examining the scatterplot of “residuals versus fits”; the variance of the residuals should be the same across all values of the x-axis. If the plot shows a pattern (e.g., bowtie or megaphone shape), then variances are not consistent, and this assumption has not been met."
      ],
      "metadata": {
        "id": "MeTRzXyBLgQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3.What does coefficient m represent in the equation of Y=mX + c ?**\n",
        "\n",
        "-->The coefficient m i the equation Y=mX = c represents the Slope,\n",
        "The slope indicates the steepness and direction of the line. It's calculated as the ratio of the change in the y-coordinate to the change in the x-coordinate between any two points on the line. A positive slope means the line rises from left to right, a negative slope means it falls from left to right, and a zero slope indicates a horizontal line.\n",
        "Y-intercept (c):"
      ],
      "metadata": {
        "id": "jJr_QqiTMb74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4.What does intercept c represents in the equation Y= mX + c ?**\n",
        "\n",
        "-->The c in the given equation represents the Y intercept.The y-intercept is the point where the line intersects the y-axis. It's the value of 'y' when 'x' is equal to zero."
      ],
      "metadata": {
        "id": "13JoIlgbNN1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5.How do we calculate the slope m in the simple linear regression ?**\n",
        "\n",
        "-->The slope m in simple linear regression, represented by the equation y = mx + b, is calculated using the formula: m = (n * Σxy - Σx * Σy) / (n * Σx² - (Σx)²). Here, n is the number of data points, Σxy is the sum of the product of x and y values, Σx is the sum of x values, Σy is the sum of y values, and Σx² is the sum of the squares of x values."
      ],
      "metadata": {
        "id": "G-p7ED7zNxZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6.What is the purpose of the least squares method in the simple linear regression ?**\n",
        "\n",
        "-->The primary purpose of least squares regression in linear regression is to find the \"best fit\" line for a set of data points by minimizing the sum of the squared differences between the observed values and the values predicted by the line."
      ],
      "metadata": {
        "id": "ddaXYvJZORKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7.How is the coefficient of determination (R²) interpretated in the simple linear regression ?**\n",
        "\n",
        "-->The coefficient of determination (R²) is a number between 0 and 1 that measures how well a statistical model predicts an outcome. You can interpret the R² as the proportion of variation in the dependent variable that is predicted by the statistical model."
      ],
      "metadata": {
        "id": "uNHbBstcQoRK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8.What is Multiple linear regrssion ?**\n",
        "\n",
        "-->Multiple linear regression is a statistical method used to model the relationship between one dependent variable and two or more independent variables. It's an extension of simple linear regression that allows for more accurate predictions by considering multiple predictors."
      ],
      "metadata": {
        "id": "1SKoKz5zRS3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9.What is the difference between the simple linear regression and Multiple linear regression ?**\n",
        "\n",
        "-->Simple linear regression models the relationship between one independent variable and one dependent variable, while multiple linear regression models the relationship between multiple independent variables and one dependent variable. Simple linear regression is used when there's a single factor influencing the outcome, whereas multiple linear regression is used when multiple factors need to be considered."
      ],
      "metadata": {
        "id": "6o9_E86XRlbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10.What are the key assumptions of multiple linear regression ?**\n",
        "\n",
        "-->Assumptions of Multiple linear regression ad\n",
        "1. Linear relationship: There exists a linear relationship between each predictor variable and the response variable.\n",
        "\n",
        "2. No Multicollinearity: None of the predictor variables are highly correlated with each other.\n",
        "\n",
        "3. Independence: The observations are independent.\n",
        "\n",
        "4. Homoscedasticity: The residuals have constant variance at every point in the linear model.\n",
        "\n",
        "5. Multivariate Normality: The residuals of the model are normally distributed.\n",
        "\n"
      ],
      "metadata": {
        "id": "PYSgpQlVR7wg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q11.What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model ?**\n",
        "\n",
        "-->Heteroscedasticity in a multiple linear regression model refers to the condition where the variance (or spread) of the error terms (residuals) is not constant across all values of the independent variables. This means that the errors are not equally scattered around the regression line."
      ],
      "metadata": {
        "id": "r8xWFYUhzGEo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q12.How can you improve a Multiple Linear Regression model with high multicollinearity ?**\n",
        "\n",
        "-->To improve a multiple linear regression model with high multicollinearity, consider these strategies: removing correlated variables, using robust regression techniques like ridge or lasso, or applying dimensionality reduction methods such as Principal Component Analysis (PCA)."
      ],
      "metadata": {
        "id": "AeykxlBVzqXU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q13.What are some common techniques for transforming categorical variables for use in regression models ?**\n",
        "\n",
        "-->Several techniques can transform categorical variables for use in regression models. One-hot encoding creates binary columns for each category, while label encoding assigns unique integer values. Feature hashing can reduce dimensionality for high-cardinality variables, and target encoding uses the target variable's mean for each category."
      ],
      "metadata": {
        "id": "-t5PenTw0FEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q14.What is the role of interection terms Multiple linear regression ?**\n",
        "\n",
        "-->In multiple linear regression, interaction terms reveal whether the effect of one independent variable on the dependent variable changes depending on the values of another independent variable. They essentially examine if the relationship between two variables is moderated by a third variable, allowing for more flexible modeling and potentially better predictive performance."
      ],
      "metadata": {
        "id": "pshs93cg0nEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q15.How can the interpretation intercept differ between the simple and multiple linear regression ?**\n",
        "\n",
        "-->In simple linear regression, the intercept represents the predicted value of the dependent variable when the independent variable is zero. In multiple linear regression, the intercept represents the predicted value of the dependent variable when all independent variables are zero, according to Investopedia and Stat Trek. However, the interpretation of the intercept in multiple regression can be more nuanced because it's influenced by the relationships between multiple independent variables."
      ],
      "metadata": {
        "id": "fXUZxPdc1XNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q16.What is the significance of slope in regression analysis and how does it affect the predictions ?**\n",
        "\n",
        "-->In regression analysis, the slope represents the change in the dependent variable for each unit change in the independent variable. It's crucial for understanding the relationship between variables and for making predictions. A larger slope indicates a stronger relationship, and the direction (positive or negative) determines whether the variables move in the same direction or opposite directions."
      ],
      "metadata": {
        "id": "QHW268fj1z9-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q17.How does the intercept in the regression model provide context for relationship between variables?**\n",
        "\n",
        "-->The intercept in a regression model, representing the predicted value of the dependent variable when the independent variable(s) are zero, provides crucial context for understanding the relationship between variables. It acts as a baseline or starting point, allowing for meaningful interpretation of the relationship when independent variables have a value of zero."
      ],
      "metadata": {
        "id": "YrP9VVUP2Ls3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q18.What are the limitations of using R² as a sole measure of model performnace ?**\n",
        "\n",
        "-->R-squared, while useful, has limitations as a sole measure of model performance. It doesn't guarantee predictive accuracy, can be misleading in complex models with many variables (leading to overfitting), and may not be suitable for non-linear relationships. It also doesn't reveal whether a model is biased or if the data and predictions are reliable."
      ],
      "metadata": {
        "id": "a0zg5jIJ2p2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q19.How would you interpret a large standard error for a regression coefficient ?**\n",
        "\n",
        "-->A large standard error for a regression coefficient suggests that the estimated coefficient is highly variable across different samples, meaning the estimate is less precise and less reliable. It indicates that there is a significant amount of uncertainty about the true population value of the coefficient."
      ],
      "metadata": {
        "id": "-XeNBxK_Eqcr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q20.How can heteroscedasticity be identified in residuals plots, and why is it important to address it ?**\n",
        "\n",
        "-->Heteroscedasticity, or non-constant variance of residuals, in a residual plot is identified by a distinct pattern, often resembling a funnel or cone shape, where the spread of residuals increases or decreases with the fitted values. It's important to address heteroscedasticity because it violates the assumptions of ordinary least squares (OLS) regression, leading to unreliable standard errors, confidence intervals, and hypothesis tests."
      ],
      "metadata": {
        "id": "ZBfOkX45Fd1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R² ?**\n",
        "\n",
        "-->r-squared measures the proportion of variance in the dependent variable explained by the independent variables, it always increases when more predictors are added. Adjusted r-squared adjusts for the number of predictors and decreases if the additional variables do not contribute to the model's significance."
      ],
      "metadata": {
        "id": "bjUiNlPdGQtZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q22.Why is it important scale variables in multiple linear regression ?**\n",
        "\n",
        "-->Scaling variables in multiple linear regression is crucial for several reasons, including improving model performance, simplifying coefficient interpretation, and ensuring the model's stability. When variables have vastly different scales, it can lead to challenges in model training, coefficient interpretation, and the overall reliability of the regression results."
      ],
      "metadata": {
        "id": "oRrpd94YGlp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q23.What is polynomial regression ?**\n",
        "\n",
        "--> polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as a polynomial in x. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, denoted E(y |x). Although polynomial regression fits a nonlinear model to the data, as a statistical estimation problem it is linear, in the sense that the regression function E(y | x) is linear in the unknown parameters that are estimated from the data."
      ],
      "metadata": {
        "id": "alSnmPMsG_DD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q24.How does polynomial regression differ from linear regression ?**\n",
        "\n",
        "-->Key Differences are given as -\n",
        "\n",
        "1. Relationship:\n",
        "Linear regression assumes a linear relationship between the independent and dependent variables, while polynomial regression allows for non-linear relationships.\n",
        "2. Model Equation:\n",
        "Linear regression uses a simple equation like y = mx + b, where 'm' is the slope and 'b' is the y-intercept. Polynomial regression can have more complex equations, like y = a + bx + cx² + dx³ + ..., where 'a', 'b', 'c', and 'd' are coefficients and 'x' is the independent variable.\n",
        "3. Data Fit:\n",
        "Linear regression fits a straight line to the data, while polynomial regression fits a curve (or polynomial) to the data, potentially providing a better fit when the relationship is not linear.\n",
        "4. Overfitting:\n",
        "Polynomial regression is more prone to overfitting if the degree of the polynomial is too high, as it might capture noise in the data. Choosing the right degree is crucial."
      ],
      "metadata": {
        "id": "er7xOh4MHWiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q25.When is polynomail regression used ?**\n",
        "\n",
        "-->Polynomial regression is a technique we can use to fit a regression model when the relationship between the predictor variable(s) and the response variable is nonlinear.\n",
        "\n",
        "A polynomial regression model takes the following form:\n",
        "\n",
        "Y = β0 + β1X + β2X2 + … + βhXh + ε"
      ],
      "metadata": {
        "id": "L3VAqtRmIIY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q26.What is general equation for polynomial regression ?**\n",
        "\n",
        "-->The general equation for polynomial regression of degree n is:\n",
        "\n",
        "y = β₀ + β₁x + β₂x² + ... + βₙxⁿ + ε\n",
        "\n",
        "where:\n",
        "\n",
        "* y: is the dependent variable (what you are trying to predict)\n",
        "* x: is the independent variable (the predictor)\n",
        "* β₀, β₁, β₂, ..., βₙ: are the coefficients that determine the shape and position of the polynomial curve\n",
        "ε: is the error term (the difference between the predicted value and the actual value)\n",
        "* n: is the degree of the polynomial (e.g., 1 for linear, 2 for quadratic, 3 for cubic, etc.)\n",
        "\n"
      ],
      "metadata": {
        "id": "VB86yalaIj2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q27.Can polynomial regression be applied to multiple variables ?**\n",
        "\n",
        "-->Yes, polynomial expressions can be applied to multiple variables, meaning they can involve more than one letter representing a variable. A polynomial can have terms with different variables, or the same variable raised to different powers. For example, 2x^2 + 3xy - 4y^2 is a polynomial with two variables, x and y."
      ],
      "metadata": {
        "id": "kN4OWTLfJkbn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q28.What are the limitations of polynomial regression ?**\n",
        "\n",
        "-->Given below are the limitations of polynomial regression-\n",
        "\n",
        "1. Overfitting: One of the most significant concerns with polynomial regression is overfitting. As the degree of the polynomial increases, the model becomes more flexible and can fit the noise in the data, leading to poor generalization performance. This is particularly problematic when working with small datasets or noisy data.\n",
        "\n",
        "2. Curse of Dimensionality: As the degree of the polynomial increases, the number of possible combinations of terms grows rapidly, making it more challenging to interpret the results. This is known as the curse of dimensionality. With too many terms, it becomes difficult to identify which variables are truly important and which are just noise.\n",
        "\n",
        "3. Computational Complexity: Polynomial regression can be computationally expensive, especially for high-degree polynomials or large datasets. This is because the optimization algorithm needs to search through a vast number of possible solutions to find the best fit.\n",
        "\n",
        "4. Interpretability: Polynomial regression models can be difficult to interpret, especially for high-degree polynomials. The coefficients of the polynomial equation may not have a clear physical meaning, making it challenging to understand the relationships between variables.\n",
        "\n",
        "5. Sensitivity to Initial Conditions: Polynomial regression models can be sensitive to initial conditions, which means that small changes in the starting values of the optimization algorithm can lead to significant differences in the final solution"
      ],
      "metadata": {
        "id": "17uiDF68KBwQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q29.What methods can be used to evaluate model fit when selecting the degree of a polynomial ?**\n",
        "\n",
        "-->Several methods can be used to evaluate model fit when selecting the degree of a polynomial, including visual inspection, cross-validation, and information criteria like AIC and BIC. These methods help identify overfitting and determine the optimal degree that balances model complexity and predictive accuracy.\n",
        "\n",
        "1. Visual Inspection:\n",
        "Plotting:\n",
        "Graphically compare the fit of polynomials with increasing degrees to your data. This allows for a visual assessment of how well the polynomial captures the underlying relationship in the data without overfitting.\n",
        "Residual Plots:\n",
        "Examine the residuals (the difference between the actual and predicted values) to identify patterns that suggest the model is not capturing the data well.\n",
        "2. Cross-Validation:\n",
        "K-fold Cross-Validation:\n",
        "Split your data into K subsets, train the model on K-1 subsets, and validate on the remaining subset. Repeat this process K times, and the degree with the lowest average validation error is chosen.\n",
        "Grid Search:\n",
        "Combine cross-validation with a grid search to systematically explore different polynomial degrees and evaluate their performance.\n",
        "3. Information Criteria (AIC/BIC):\n",
        "Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC): These criteria balance the goodness-of-fit of the model with its complexity (number of parameters). They help select the model with the lowest AIC or BIC score, which is a good indicator of overall model performance.\n",
        "4. Other Methods:\n",
        "Ramsey RESET Test:\n",
        "This test can be used to assess if the model is misspecified (i.e., the relationship between variables is not captured correctly by the chosen polynomial).\n",
        "Forward and Backward Selection:\n",
        "These techniques involve adding or removing polynomial terms (degrees) based on their statistical significance."
      ],
      "metadata": {
        "id": "erybguUiLXz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q30.Why is visualisation important in polynomial regression ?**\n",
        "\n",
        "-->Visualization is crucial in polynomial regression to understand data trends, assess model fit, and choose the right polynomial degree. It helps reveal the non-linear relationships that linear regression might miss, and allows for a visual check of the model's performance."
      ],
      "metadata": {
        "id": "OOMwqN7oLzGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q31.How is polynomial regression implemented in python ?**\n",
        "\n",
        "-->Polynomial regression is an extension of linear regression where the relationship between the independent variable\n",
        "𝑥\n",
        "x and the dependent variable\n",
        "𝑦\n",
        "y is modeled as an\n",
        "𝑛\n",
        "n-degree polynomial. In Python, it's commonly implemented using scikit-learn."
      ],
      "metadata": {
        "id": "_I_DiPEvMMao"
      }
    }
  ]
}